{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivam361v/ComputerVisionAndImageProcessing/blob/main/DrowsynessDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE62eT9DKIbl",
        "outputId": "5a05f03a-0b9e-44f6-9e1f-5e4de25e2362"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.25.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.7-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.14 protobuf-4.25.3 sounddevice-0.4.7\n"
          ]
        }
      ],
      "source": [
        "pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVhbX2yPJ5sT"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe.python.solutions.drawing_utils import _normalized_to_pixel_coordinates as denormalize_coordinates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvUtfQ82KyUF"
      },
      "outputs": [],
      "source": [
        "def get_mediapipe_app(\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5,\n",
        "):\n",
        "    \"\"\"Initialize and return Mediapipe FaceMesh Solution Graph object\"\"\"\n",
        "    face_mesh = mp.solutions.face_mesh.FaceMesh(\n",
        "        max_num_faces=max_num_faces,\n",
        "        refine_landmarks=refine_landmarks,\n",
        "        min_detection_confidence=min_detection_confidence,\n",
        "        min_tracking_confidence=min_tracking_confidence,\n",
        "    )\n",
        "\n",
        "    return face_mesh\n",
        "\n",
        "\n",
        "def distance(point_1, point_2):\n",
        "    \"\"\"Calculate l2-norm between two points\"\"\"\n",
        "    dist = sum([(i - j) ** 2 for i, j in zip(point_1, point_2)]) ** 0.5\n",
        "    return dist\n",
        "\n",
        "\n",
        "def get_ear(landmarks, refer_idxs, frame_width, frame_height):\n",
        "    \"\"\"\n",
        "    Calculate Eye Aspect Ratio for one eye.\n",
        "\n",
        "    Args:\n",
        "        landmarks: (list) Detected landmarks list\n",
        "        refer_idxs: (list) Index positions of the chosen landmarks\n",
        "                            in order P1, P2, P3, P4, P5, P6\n",
        "        frame_width: (int) Width of captured frame\n",
        "        frame_height: (int) Height of captured frame\n",
        "\n",
        "    Returns:\n",
        "        ear: (float) Eye aspect ratio\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Compute the euclidean distance between the horizontal\n",
        "        coords_points = []\n",
        "        for i in refer_idxs:\n",
        "            lm = landmarks[i]\n",
        "            coord = denormalize_coordinates(lm.x, lm.y, frame_width, frame_height)\n",
        "            coords_points.append(coord)\n",
        "\n",
        "        # Eye landmark (x, y)-coordinates\n",
        "        P2_P6 = distance(coords_points[1], coords_points[5])\n",
        "        P3_P5 = distance(coords_points[2], coords_points[4])\n",
        "        P1_P4 = distance(coords_points[0], coords_points[3])\n",
        "\n",
        "        # Compute the eye aspect ratio\n",
        "        ear = (P2_P6 + P3_P5) / (2.0 * P1_P4)\n",
        "\n",
        "    except:\n",
        "        ear = 0.0\n",
        "        coords_points = None\n",
        "\n",
        "    return ear, coords_points\n",
        "\n",
        "\n",
        "def calculate_avg_ear(landmarks, left_eye_idxs, right_eye_idxs, image_w, image_h):\n",
        "    # Calculate Eye aspect ratio\n",
        "\n",
        "    left_ear, left_lm_coordinates = get_ear(landmarks, left_eye_idxs, image_w, image_h)\n",
        "    right_ear, right_lm_coordinates = get_ear(landmarks, right_eye_idxs, image_w, image_h)\n",
        "    Avg_EAR = (left_ear + right_ear) / 2.0\n",
        "\n",
        "    return Avg_EAR, (left_lm_coordinates, right_lm_coordinates)\n",
        "\n",
        "\n",
        "def plot_eye_landmarks(frame, left_lm_coordinates, right_lm_coordinates, color):\n",
        "    for lm_coordinates in [left_lm_coordinates, right_lm_coordinates]:\n",
        "        if lm_coordinates:\n",
        "            for coord in lm_coordinates:\n",
        "                cv2.circle(frame, coord, 2, color, -1)\n",
        "\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    return frame\n",
        "\n",
        "\n",
        "def plot_text(image, text, origin, color, font=cv2.FONT_HERSHEY_SIMPLEX, fntScale=0.8, thickness=2):\n",
        "    image = cv2.putText(image, text, origin, font, fntScale, color, thickness)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAYBCR9GLCAN"
      },
      "outputs": [],
      "source": [
        "class VideoFrameHandler:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the necessary constants, mediapipe app\n",
        "        and tracker variables\n",
        "        \"\"\"\n",
        "        # Left and right eye chosen landmarks.\n",
        "        self.eye_idxs = {\n",
        "            \"left\": [362, 385, 387, 263, 373, 380],\n",
        "            \"right\": [33, 160, 158, 133, 153, 144],\n",
        "        }\n",
        "\n",
        "        # Used for coloring landmark points.\n",
        "        # Its value depends on the current EAR value.\n",
        "        self.RED = (0, 0, 255)  # BGR\n",
        "        self.GREEN = (0, 255, 0)  # BGR\n",
        "\n",
        "        # Initializing Mediapipe FaceMesh solution pipeline\n",
        "        self.facemesh_model = get_mediapipe_app()\n",
        "\n",
        "        # For tracking counters and sharing states in and out of callbacks.\n",
        "        self.state_tracker = {\n",
        "            \"start_time\": time.perf_counter(),\n",
        "            \"DROWSY_TIME\": 0.0,  # Holds time passed with EAR < EAR_THRESH\n",
        "            \"COLOR\": self.GREEN,\n",
        "            \"play_alarm\": False,\n",
        "        }\n",
        "\n",
        "        self.EAR_txt_pos = (10, 30)\n",
        "        \n",
        "    def process(self, frame: np.array, thresholds: dict):\n",
        "        \"\"\"\n",
        "        This function is used to implement our Drowsy detection algorithm.\n",
        "\n",
        "        Args:\n",
        "            frame: (np.array) Input frame matrix.\n",
        "            thresholds: (dict) Contains the two threshold values\n",
        "                               WAIT_TIME and EAR_THRESH.\n",
        "\n",
        "        Returns:\n",
        "            The processed frame and a boolean flag to\n",
        "            indicate if the alarm should be played or not.\n",
        "        \"\"\"\n",
        "\n",
        "        # To improve performance,\n",
        "        # mark the frame as not writeable to pass by reference.\n",
        "        frame.flags.writeable = False\n",
        "        frame_h, frame_w, _ = frame.shape\n",
        "        DROWSY_TIME_txt_pos = (10, int(frame_h // 2 * 1.7))\n",
        "        ALM_txt_pos = (10, int(frame_h // 2 * 1.85))\n",
        "\n",
        "        results = self.facemesh_model.process(frame)\n",
        "\n",
        "        if results.multi_face_landmarks:\n",
        "            landmarks = results.multi_face_landmarks[0].landmark\n",
        "            EAR, coordinates = calculate_avg_ear(landmarks,\n",
        "                                                 self.eye_idxs[\"left\"],\n",
        "                                                 self.eye_idxs[\"right\"],\n",
        "                                                 frame_w,\n",
        "                                                 frame_h\n",
        "                                                 )\n",
        "            frame = plot_eye_landmarks(frame,\n",
        "                                       coordinates[0],\n",
        "                                       coordinates[1],\n",
        "                                       self.state_tracker[\"COLOR\"]\n",
        "                                       )\n",
        "\n",
        "            if EAR < thresholds[\"EAR_THRESH\"]:\n",
        "\n",
        "                # Increase DROWSY_TIME to track the time period with\n",
        "                # EAR less than the threshold\n",
        "                # and reset the start_time for the next iteration.\n",
        "                end_time = time.perf_counter()\n",
        "\n",
        "                self.state_tracker[\"DROWSY_TIME\"] += end_time - self.state_tracker[\"start_time\"]\n",
        "                self.state_tracker[\"start_time\"] = end_time\n",
        "                self.state_tracker[\"COLOR\"] = self.RED\n",
        "\n",
        "                if self.state_tracker[\"DROWSY_TIME\"] >= thresholds[\"WAIT_TIME\"]:\n",
        "                    self.state_tracker[\"play_alarm\"] = True\n",
        "                    plot_text(frame, \"WAKE UP! WAKE UP\",\n",
        "                              ALM_txt_pos, self.state_tracker[\"COLOR\"])\n",
        "\n",
        "            else:\n",
        "                self.state_tracker[\"start_time\"] = time.perf_counter()\n",
        "                self.state_tracker[\"DROWSY_TIME\"] = 0.0\n",
        "                self.state_tracker[\"COLOR\"] = self.GREEN\n",
        "                self.state_tracker[\"play_alarm\"] = False\n",
        "\n",
        "            EAR_txt = f\"EAR: {round(EAR, 2)}\"\n",
        "            DROWSY_TIME_txt = f\"DROWSY: {round(self.state_tracker['DROWSY_TIME'], 3)} Secs\"\n",
        "            plot_text(frame, EAR_txt,\n",
        "                      self.EAR_txt_pos, self.state_tracker[\"COLOR\"])\n",
        "            plot_text(frame, DROWSY_TIME_txt,\n",
        "                      DROWSY_TIME_txt_pos, self.state_tracker[\"COLOR\"])\n",
        "\n",
        "        else:\n",
        "            self.state_tracker[\"start_time\"] = time.perf_counter()\n",
        "            self.state_tracker[\"DROWSY_TIME\"] = 0.0\n",
        "            self.state_tracker[\"COLOR\"] = self.GREEN\n",
        "            self.state_tracker[\"play_alarm\"] = False\n",
        "\n",
        "            # Flip the frame horizontally for a selfie-view display.\n",
        "            frame = cv2.flip(frame, 1)\n",
        "\n",
        "        return frame, self.state_tracker[\"play_alarm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKUPGryYLMuO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMSyrN92ILVJcx3+eoiXRLE",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
